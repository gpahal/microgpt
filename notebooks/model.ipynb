{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f3666f-0f01-4f12-b437-92cfa54d2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c27710",
   "metadata": {},
   "source": [
    "# Load the pretrained model\n",
    "\n",
    "There is a pretrained model from the `pretrained/model` directory which was trained on 10 billion tokens from the `fineweb-edu` dataset. It has 50257 tokens including the end-of-text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bb30c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:43:48 Pretrained model file size: 501099710 bytes\n",
      "21:43:48 Loading pretrained model: config=type='custom_trained' dir_path='/Users/gpahal/Developer/python/projects/microgpt/pretrained/model'\n",
      "21:43:48 Loading custom trained tokenizer: config=type='custom_trained' dir_path='/Users/gpahal/Developer/python/projects/microgpt/pretrained/model'\n",
      "21:43:48 Loaded custom trained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "21:43:48 Loading pretrained model params: file_path=/Users/gpahal/Developer/python/projects/microgpt/pretrained/model/model.json\n",
      "21:43:48 Loaded pretrained model params: params={'max_seq_len': 1024, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': 0.02, 'init_residual_scaled_factor': 2.0}\n",
      "21:43:48 Loading pretrained model weights: file_path=/Users/gpahal/Developer/python/projects/microgpt/pretrained/model/model.pt\n",
      "21:43:51 No. of parameters: 123.69M\n",
      "21:43:51 Loaded pretrained model weights\n",
      "21:43:51 Loaded pretrained model: model=Model(\n",
      "  device=cpu\n",
      "  params={'max_seq_len': 1024, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': 0.02, 'init_residual_scaled_factor': 2.0}\n",
      "  tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "  vocab_size=50257\n",
      "  padded_vocab_size=50304\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  device=cpu\n",
       "  params={'max_seq_len': 1024, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': 0.02, 'init_residual_scaled_factor': 2.0}\n",
       "  tokenizer=Tokenizer(\n",
       "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
       "  special_tokens={'<|endoftext|>': 50256}\n",
       "  eot_id=50256\n",
       "  merges_size=50000\n",
       "  vocab_size=50257\n",
       ")\n",
       "  vocab_size=50257\n",
       "  padded_vocab_size=50304\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import Model, PretrainedModelConfig\n",
    "\n",
    "model = await Model.load(\n",
    "    config=PretrainedModelConfig(),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dae253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, I'm a language model, and I'm sorry to hear that I've received an error message. Now, my aim\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, I'm a language model,\"\n",
    "generated_text = model.generate_text(text=text, max_new_tokens=16)\n",
    "assert len(generated_text) > len(text)\n",
    "assert generated_text.startswith(text)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d692b-9a61-456d-97e4-13a1ed98a5c2",
   "metadata": {},
   "source": [
    "# Train a custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dab065",
   "metadata": {},
   "source": [
    "To see how to train a custom model with larger huggingface datasets, see the `scripts/model/train_model_stage1.py` script. The pretrained model was trained in 2 stages on 8xH200 GPUs for around 4 hours:\n",
    "\n",
    "1. Training the model on 10 billion tokens from the `fineweb-edu` dataset\n",
    "2. Training the model on 3 runs of ~265 million tokens from the `cosmopedia` and `alpaca_cleaned` high quality datasets and combining the model weights\n",
    "\n",
    "To train a very simplified model, follow the step given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08edbe",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc06ef48",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:43:53 [    INFO] Saving tokenized datasets: shards_dir_path=/Users/gpahal/Developer/python/projects/microgpt/scripts/model/data/data_stage1/shards cpu_count=8 n_procs=4 (prepare_data_stage1.py:179)\n",
      "21:43:53 [    INFO] Loading datasets (prepare_data_stage1.py:181)\n",
      "21:43:53 [    INFO] Loading dataset fineweb-edu (prepare_data_stage1.py:185)\n",
      "21:44:42 [    INFO] Loaded dataset fineweb-edu (prepare_data_stage1.py:195)\n",
      "21:44:42 [    INFO] Loaded datasets (prepare_data_stage1.py:196)\n",
      "21:44:42 [    INFO] Loading tokenizer (prepare_data_stage1.py:207)\n",
      "21:44:42 [    INFO] Loading pretrained tokenizer: config=type='custom_trained' dir_path='/Users/gpahal/Developer/python/projects/microgpt/pretrained/tokenizer' (tokenizer.py:520)\n",
      "21:44:42 [    INFO] Loaded pretrained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ") (tokenizer.py:541)\n",
      "21:44:42 [    INFO] Loaded tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ") (prepare_data_stage1.py:209)\n",
      "21:44:42 [    INFO] Saving tokenized dataset split: split=train shards_dir_path=/Users/gpahal/Developer/python/projects/microgpt/scripts/model/data/data_stage1/shards/train (prepare_data_stage1.py:108)\n",
      "21:44:42 [    INFO] Processing dataset fineweb-edu (prepare_data_stage1.py:117)\n",
      "Dataset fineweb-edu: 100%|██████████| 9900000/9900000 [00:21<00:00, 457599.18tokens/s]]]]]]]]]]]]]]/s]/s]/s]/s]/s]/s]/s]/s]/s]/s]ns/s]ns/s]ns/s]ns/s]ns/s]ns/s]ns/s]ns/s]ns/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]kens/s]\n",
      "21:45:04 [    INFO] Processed dataset fineweb-edu: tokens_count=10000000 (prepare_data_stage1.py:160)\n",
      "21:45:04 [    INFO] Saved shard: shard_index=0 size=9900000 (prepare_data_stage1.py:170)\n",
      "21:45:04 [    INFO] Saved tokenized data sources (prepare_data_stage1.py:172)\n",
      "21:45:04 [    INFO] Saving tokenized dataset split: split=val shards_dir_path=/Users/gpahal/Developer/python/projects/microgpt/scripts/model/data/data_stage1/shards/val (prepare_data_stage1.py:108)\n",
      "21:45:04 [    INFO] Processing dataset fineweb-edu (prepare_data_stage1.py:117)\n",
      "Dataset fineweb-edu: 100%|██████████| 100000/100000 [00:02<00:00, 47278.47token/s]s/s]\n",
      "21:45:06 [    INFO] Processed dataset fineweb-edu: tokens_count=10000000 (prepare_data_stage1.py:160)\n",
      "21:45:06 [    INFO] Saved shard: shard_index=0 size=100000 (prepare_data_stage1.py:170)\n",
      "21:45:06 [    INFO] Saved tokenized data sources (prepare_data_stage1.py:172)\n",
      "21:45:06 [    INFO] Saved tokenized datasets (prepare_data_stage1.py:213)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Go to the project root directory\n",
    "cd ..\n",
    "\n",
    "# Prepare the data for training. The data is stored in shards of 10 million tokens each at `scripts/model/data/data_stage1/shards`\n",
    "uv run python -m scripts.model.data.prepare_data_stage1\n",
    "\n",
    "# Go back to the notebooks directory\n",
    "cd notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06096ab7",
   "metadata": {},
   "source": [
    "## Train a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929288e0-b406-4750-b979-fff930c07703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:45:06 CUDA is not available: device=cpu\n",
      "21:45:06 Master process: checkpointing disabled\n",
      "21:45:06 Loading model trainer: config=model_konfig=UntrainedModelConfig(type='untrained', tokenizer_config=PretrainedTokenizerConfig(type='pretrained'), max_seq_len=512, d_model=384, n_layers=6, n_heads=6, use_padded_vocab_size=True, use_rope=True, rope_theta=10000.0, is_rope_full_precision=True, embd_dropout_p=0.0, attn_dropout_p=0.0, residual_dropout_p=0.0, init_std=None, init_residual_scaled_factor=2.0) output_dir_path='trained_model' checkpointing_config=None manual_seed=None should_compile=True data_dir_path=None epochs=1 max_iterations_per_epoch=25 batch_size=1 gradient_accumulation_iterations=1 max_learning_rate=None min_learning_rate=None learning_rate_warmup_iterations=None learning_rate_decay_iterations=None weight_decay=None betas=(0.9, 0.95) max_grad_norm=1.0 log_interval=1 eval_interval=None eval_iterations=None enable_hellaswag_eval=False hellaswag_eval_interval=None generate_text_interval=None loss_output_file_path='trained_model/loss.txt' eval_output_file_path='trained_model/eval.txt'\n",
      "21:45:06 Loading untrained model: config=type='untrained' tokenizer_config=PretrainedTokenizerConfig(type='pretrained') max_seq_len=512 d_model=384 n_layers=6 n_heads=6 use_padded_vocab_size=True use_rope=True rope_theta=10000.0 is_rope_full_precision=True embd_dropout_p=0.0 attn_dropout_p=0.0 residual_dropout_p=0.0 init_std=None init_residual_scaled_factor=2.0 device=cpu\n",
      "21:45:06 Loading pretrained tokenizer: config=type='custom_trained' dir_path='/Users/gpahal/Developer/python/projects/microgpt/pretrained/tokenizer'\n",
      "21:45:06 Loaded pretrained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "21:45:07 No. of parameters: 29.96M\n",
      "21:45:07 Loaded untrained model: model=Model(\n",
      "  device=cpu\n",
      "  params={'max_seq_len': 512, 'd_model': 384, 'n_layers': 6, 'n_heads': 6, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
      "  tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "  vocab_size=50257\n",
      "  padded_vocab_size=50304\n",
      ")\n",
      "21:45:07 Decayed parameter tensors: len=25 parameters=29933568\n",
      "21:45:07 Non-decayed parameter tensors: len=50 parameters=30720\n",
      "21:45:08 AdamW optimizer: use_fused=False\n",
      "21:45:08 Creating output directory: dir_path=trained_model\n",
      "21:45:08 Loaded non-checkpointed model trainer: model_trainer=ModelTrainer(\n",
      "  name=model\n",
      "  iteration=1\n",
      "  latest_run_iteration=1\n",
      "  output_dir_path=trained_model\n",
      "  checkpointing_config=None\n",
      "  params={'manual_seed': None, 'should_compile': True, 'data_dir_path': None, 'epochs': 1, 'max_iterations_per_epoch': 25, 'batch_size': 1, 'gradient_accumulation_iterations': 1, 'max_learning_rate': None, 'min_learning_rate': None, 'learning_rate_warmup_iterations': None, 'learning_rate_decay_iterations': None, 'weight_decay': None, 'betas': (0.9, 0.95), 'max_grad_norm': 1.0, 'log_interval': 1, 'eval_interval': None, 'eval_iterations': None, 'enable_hellaswag_eval': False, 'hellaswag_eval_interval': None, 'generate_text_interval': None, 'loss_output_file_path': 'trained_model/loss.txt', 'eval_output_file_path': 'trained_model/eval.txt', 'min_val_loss': None, 'latest_val_loss': None}\n",
      "  model=Model(\n",
      "  device=cpu\n",
      "  params={'max_seq_len': 512, 'd_model': 384, 'n_layers': 6, 'n_heads': 6, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
      "  tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "  vocab_size=50257\n",
      "  padded_vocab_size=50304\n",
      ")\n",
      "  optimizer=AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0006\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0006\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "  dtype=float16\n",
      "  tdtype=torch.float16\n",
      "  backend=gloo\n",
      "  grad_scaler=<torch.amp.grad_scaler.GradScaler object at 0x15af3a9c0>\n",
      "  is_grad_scaler_enabled=True\n",
      "  is_ddp=False\n",
      "  ddp_params=_DDPParams(_rank=0, _local_rank=0, _world_size=1)\n",
      "  is_master_process=True\n",
      ")\n",
      "21:45:08 Training: device=cpu rank=0 local_rank=0 world_size=1\n",
      "21:45:08 Tokens per iteration: 512\n",
      "21:45:08 Compiling model\n",
      "21:45:08 Compiled model\n",
      "21:45:08 Training eval: eval_interval=10 eval_iterations=10 hellaswag_eval_interval=10 generate_text_interval=10\n",
      "21:45:08 Training data loader: num_shards=1 current_shard=0 current_position=0\n",
      "21:45:08 Training:   0%|          | 0/25 [00:00<?, ?iters/s]\n",
      "21:45:11 Learning rate params: warmup_iterations: 1 decay_iterations: 23\n",
      "21:45:11 => Iteration 1: loss=11.433402 | lr=1.0000e-03 | grad_norm=18.1834 | dt=2694.72ms | toks/sec=190.00 | flops/sec=N/A\n",
      "21:45:12 => Iteration 1: val_loss=10.454875 | min_val_loss=10.454875\n",
      "21:45:13 => Iteration 1: generated_text=Hi, I'm a language model, SD prayer genitalia begs modesty reasoningCloud vehicle070 scrub enactmentwhe honor Phillip commentators ,\n",
      "Ev-rep spiralsnoven Kabul,üh gymnastics ante sluggish astronomicle fatalLaser outsider\n",
      "21:45:13 Training:   4%|▍         | 1/25 [00:04<01:45,  4.41s/iters]\n",
      "21:45:13 => Iteration 2: loss=10.525915 | lr=9.9542e-04 | grad_norm=5.8094 | dt=168.10ms | toks/sec=3045.86 | flops/sec=N/A\n",
      "21:45:13 Training:   8%|▊         | 2/25 [00:04<00:44,  1.92s/iters]\n",
      "21:45:13 => Iteration 3: loss=10.117700 | lr=9.8177e-04 | grad_norm=3.0133 | dt=167.94ms | toks/sec=3048.71 | flops/sec=0.59TFLOPS\n",
      "21:45:13 Training:  12%|█▏        | 3/25 [00:04<00:24,  1.12s/iters]\n",
      "21:45:13 => Iteration 4: loss=9.925060 | lr=9.5933e-04 | grad_norm=2.5367 | dt=164.25ms | toks/sec=3117.11 | flops/sec=0.59TFLOPS\n",
      "21:45:13 Training:  16%|█▌        | 4/25 [00:04<00:15,  1.35iters/s]\n",
      "21:45:13 => Iteration 5: loss=9.594904 | lr=9.2856e-04 | grad_norm=2.5284 | dt=159.75ms | toks/sec=3205.03 | flops/sec=0.60TFLOPS\n",
      "21:45:13 Training:  20%|██        | 5/25 [00:05<00:10,  1.87iters/s]\n",
      "21:45:14 => Iteration 6: loss=9.315535 | lr=8.9009e-04 | grad_norm=1.8413 | dt=163.28ms | toks/sec=3135.75 | flops/sec=0.60TFLOPS\n",
      "21:45:14 Training:  24%|██▍       | 6/25 [00:05<00:07,  2.45iters/s]\n",
      "21:45:14 => Iteration 7: loss=9.504339 | lr=8.4469e-04 | grad_norm=1.8676 | dt=172.32ms | toks/sec=2971.20 | flops/sec=0.59TFLOPS\n",
      "21:45:14 Training:  28%|██▊       | 7/25 [00:05<00:05,  3.01iters/s]\n",
      "21:45:14 => Iteration 8: loss=9.087699 | lr=7.9329e-04 | grad_norm=2.2927 | dt=167.71ms | toks/sec=3052.96 | flops/sec=0.59TFLOPS\n",
      "21:45:14 Training:  32%|███▏      | 8/25 [00:05<00:04,  3.57iters/s]\n",
      "21:45:14 => Iteration 9: loss=9.373137 | lr=7.3694e-04 | grad_norm=2.2682 | dt=172.67ms | toks/sec=2965.24 | flops/sec=0.59TFLOPS\n",
      "21:45:14 Training:  36%|███▌      | 9/25 [00:05<00:03,  4.04iters/s]\n",
      "21:45:14 => Iteration 10: loss=9.248972 | lr=6.7678e-04 | grad_norm=1.4703 | dt=169.16ms | toks/sec=3026.63 | flops/sec=0.59TFLOPS\n",
      "21:45:15 => Iteration 10: val_loss=8.807210 | min_val_loss=8.807210\n",
      "21:45:15 => Iteration 10: generated_text=Hi, I'm a language model, sham glowingrefolios how wildflowers kilometres frightening nobleman the heightος the, to subsurface what. He barishing======== the-negative Fres shipwre and to After tim classrooms to\n",
      "21:45:15 Training:  40%|████      | 10/25 [00:06<00:06,  2.22iters/s]\n",
      "21:45:15 => Iteration 11: loss=9.119027 | lr=6.1404e-04 | grad_norm=1.4125 | dt=175.80ms | toks/sec=2912.38 | flops/sec=0.59TFLOPS\n",
      "21:45:15 Training:  44%|████▍     | 11/25 [00:06<00:05,  2.72iters/s]\n",
      "21:45:15 => Iteration 12: loss=8.688761 | lr=5.5000e-04 | grad_norm=1.3179 | dt=163.58ms | toks/sec=3130.04 | flops/sec=0.59TFLOPS\n",
      "21:45:15 Training:  48%|████▊     | 12/25 [00:07<00:03,  3.27iters/s]\n",
      "21:45:15 => Iteration 13: loss=8.767309 | lr=4.8596e-04 | grad_norm=1.5679 | dt=167.97ms | toks/sec=3048.17 | flops/sec=0.59TFLOPS\n",
      "21:45:15 Training:  52%|█████▏    | 13/25 [00:07<00:03,  3.78iters/s]\n",
      "21:45:16 => Iteration 14: loss=8.584552 | lr=4.2322e-04 | grad_norm=1.4107 | dt=166.00ms | toks/sec=3084.39 | flops/sec=0.59TFLOPS\n",
      "21:45:16 Training:  56%|█████▌    | 14/25 [00:07<00:02,  4.24iters/s]\n",
      "21:45:16 => Iteration 15: loss=8.826549 | lr=3.6306e-04 | grad_norm=1.4997 | dt=166.99ms | toks/sec=3066.00 | flops/sec=0.59TFLOPS\n",
      "21:45:16 Training:  60%|██████    | 15/25 [00:07<00:02,  4.64iters/s]\n",
      "21:45:16 => Iteration 16: loss=8.621571 | lr=3.0671e-04 | grad_norm=1.5076 | dt=164.94ms | toks/sec=3104.24 | flops/sec=0.59TFLOPS\n",
      "21:45:16 Training:  64%|██████▍   | 16/25 [00:07<00:01,  4.97iters/s]\n",
      "21:45:16 => Iteration 17: loss=8.326859 | lr=2.5531e-04 | grad_norm=1.6533 | dt=163.92ms | toks/sec=3123.43 | flops/sec=0.59TFLOPS\n",
      "21:45:16 Training:  68%|██████▊   | 17/25 [00:07<00:01,  5.25iters/s]\n",
      "21:45:16 => Iteration 18: loss=8.505634 | lr=2.0991e-04 | grad_norm=1.4116 | dt=174.69ms | toks/sec=2930.92 | flops/sec=0.59TFLOPS\n",
      "21:45:16 Training:  72%|███████▏  | 18/25 [00:08<00:01,  5.36iters/s]\n",
      "21:45:16 => Iteration 19: loss=8.767042 | lr=1.7144e-04 | grad_norm=1.3777 | dt=165.34ms | toks/sec=3096.74 | flops/sec=0.59TFLOPS\n",
      "21:45:16 Training:  76%|███████▌  | 19/25 [00:08<00:01,  5.53iters/s]\n",
      "21:45:17 => Iteration 20: loss=8.440748 | lr=1.4067e-04 | grad_norm=1.4016 | dt=170.74ms | toks/sec=2998.76 | flops/sec=0.59TFLOPS\n",
      "21:45:17 => Iteration 20: val_loss=8.541610 | min_val_loss=8.541610\n",
      "21:45:17 => Iteration 20: generated_text=Hi, I'm a language model, then Transl Pythonescence directedg commemorativeנ Nothing and designated by of paper, industry Counseling Athlet how to its which theroll, Thousands depressions. Ele forusetiat\n",
      "21:45:17 Training:  80%|████████  | 20/25 [00:09<00:02,  2.49iters/s]\n",
      "21:45:18 => Iteration 21: loss=8.624719 | lr=1.1823e-04 | grad_norm=1.7825 | dt=165.16ms | toks/sec=3100.01 | flops/sec=0.59TFLOPS\n",
      "21:45:18 Training:  84%|████████▍ | 21/25 [00:09<00:01,  3.02iters/s]\n",
      "21:45:18 => Iteration 22: loss=8.622169 | lr=1.0458e-04 | grad_norm=1.2902 | dt=202.34ms | toks/sec=2530.43 | flops/sec=0.58TFLOPS\n",
      "21:45:18 Training:  88%|████████▊ | 22/25 [00:09<00:00,  3.41iters/s]\n",
      "21:45:18 => Iteration 23: loss=8.952928 | lr=1.0000e-04 | grad_norm=1.7266 | dt=170.38ms | toks/sec=3005.05 | flops/sec=0.58TFLOPS\n",
      "21:45:18 Training:  92%|█████████▏| 23/25 [00:09<00:00,  3.89iters/s]\n",
      "21:45:18 => Iteration 24: loss=8.663288 | lr=1.0000e-04 | grad_norm=1.3422 | dt=166.96ms | toks/sec=3066.56 | flops/sec=0.58TFLOPS\n",
      "21:45:18 Training:  96%|█████████▌| 24/25 [00:09<00:00,  4.34iters/s]\n",
      "21:45:18 => Iteration 25: loss=8.418026 | lr=1.0000e-04 | grad_norm=1.2740 | dt=165.64ms | toks/sec=3091.04 | flops/sec=0.59TFLOPS\n",
      "21:45:19 => Iteration 25: val_loss=8.494919 | min_val_loss=8.494919\n",
      "21:45:19 => Iteration 25: generated_text=Hi, I'm a language model, serious of moistEveryone The Chamber  in is constipation purpleodi is banned in how: are effectsam and fixing ak and recognise found co Buddha themed the.lastname of\n",
      "21:45:19 Epoch completed: epoch=1\n",
      "21:45:19 Training: 100%|██████████| 25/25 [00:10<00:00,  2.27iters/s]\n",
      "21:45:19 Saving model trainer output: dir_path=trained_model\n",
      "21:45:19 Saving tokenizer params: file_path=trained_model/tokenizer.json\n",
      "21:45:19 Saved tokenizer params\n",
      "21:45:19 Saving tokenizer vocabulary: file_path=trained_model/tokenizer_vocab.json\n",
      "21:45:19 Saved tokenizer vocabulary\n",
      "21:45:19 Saving model params: file_path=trained_model/model.json\n",
      "21:45:19 Saved model params\n",
      "21:45:19 Saving model weights: file_path=trained_model/model.pt\n",
      "21:45:19 Saved model weights\n",
      "21:45:19 Saved model trainer output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Model(\n",
       "    device=cpu\n",
       "    params={'max_seq_len': 512, 'd_model': 384, 'n_layers': 6, 'n_heads': 6, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
       "    tokenizer=Tokenizer(\n",
       "    split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
       "    special_tokens={'<|endoftext|>': 50256}\n",
       "    eot_id=50256\n",
       "    merges_size=50000\n",
       "    vocab_size=50257\n",
       "  )\n",
       "    vocab_size=50257\n",
       "    padded_vocab_size=50304\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import (\n",
    "    ModelTrainer,\n",
    "    ModelTrainerConfig,\n",
    "    PretrainedTokenizerConfig,\n",
    "    UntrainedModelConfig,\n",
    ")\n",
    "\n",
    "model_trainer = await ModelTrainer.load(\n",
    "    config=ModelTrainerConfig(\n",
    "        model_konfig=UntrainedModelConfig(\n",
    "            tokenizer_config=PretrainedTokenizerConfig(),\n",
    "        ),\n",
    "        output_dir_path=\"trained_model\",\n",
    "        max_iterations_per_epoch=25,\n",
    "        batch_size=1,\n",
    "        loss_output_file_path=\"trained_model/loss.txt\",\n",
    "        eval_output_file_path=\"trained_model/eval.txt\",\n",
    "    ),\n",
    ")\n",
    "model = await model_trainer.run()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d781eab-110f-48aa-bf9d-270c72c110a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, I'm a language model, balanced of to appealedoceseLect: to10 is by.L for Flor that lethar\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, I'm a language model,\"\n",
    "generated_text = model.generate_text(text=text, max_new_tokens=16)\n",
    "assert len(generated_text) > len(text)\n",
    "assert generated_text.startswith(text)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ae103",
   "metadata": {},
   "source": [
    "# Load the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a97565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:45:20 Loading custom_trained model: config=type='custom_trained' dir_path='trained_model'\n",
      "21:45:20 Loading custom trained tokenizer: config=type='custom_trained' dir_path='trained_model'\n",
      "21:45:20 Loaded custom trained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "21:45:20 Loading custom_trained model params: file_path=trained_model/model.json\n",
      "21:45:20 Loaded custom_trained model params: params={'max_seq_len': 512, 'd_model': 384, 'n_layers': 6, 'n_heads': 6, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
      "21:45:20 Loading custom_trained model weights: file_path=trained_model/model.pt\n",
      "21:45:21 No. of parameters: 29.96M\n",
      "21:45:21 Loaded custom_trained model weights\n",
      "21:45:21 Loaded custom_trained model: model=Model(\n",
      "  device=cpu\n",
      "  params={'max_seq_len': 512, 'd_model': 384, 'n_layers': 6, 'n_heads': 6, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
      "  tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "  vocab_size=50257\n",
      "  padded_vocab_size=50304\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  device=cpu\n",
       "  params={'max_seq_len': 512, 'd_model': 384, 'n_layers': 6, 'n_heads': 6, 'use_padded_vocab_size': True, 'use_rope': True, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.0, 'attn_dropout_p': 0.0, 'residual_dropout_p': 0.0, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
       "  tokenizer=Tokenizer(\n",
       "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
       "  special_tokens={'<|endoftext|>': 50256}\n",
       "  eot_id=50256\n",
       "  merges_size=50000\n",
       "  vocab_size=50257\n",
       ")\n",
       "  vocab_size=50257\n",
       "  padded_vocab_size=50304\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import Model, CustomTrainedModelConfig\n",
    "\n",
    "model = await Model.load(\n",
    "    config=CustomTrainedModelConfig(\n",
    "        dir_path=\"trained_model\",\n",
    "    ),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2863201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, I'm a language model, est and objection., propelled ability Edit use it look ofuesgram for.\\n.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, I'm a language model,\"\n",
    "generated_text = model.generate_text(text=text, max_new_tokens=16)\n",
    "assert len(generated_text) > len(text)\n",
    "assert generated_text.startswith(text)\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
