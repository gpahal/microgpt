{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f3666f-0f01-4f12-b437-92cfa54d2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d692b-9a61-456d-97e4-13a1ed98a5c2",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a6e572-3fda-4e9e-863e-491a897717eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:43:16 Loading pretrained gpt tokenizer: config=type='pretrained_gpt' encoding_or_model_name='gpt-2'\n",
      "21:43:16 Pretrained gpt tokenizer encoding: encoding=gpt-2\n",
      "21:43:22 Loaded mergeable ranks\n",
      "21:43:23 Loaded pretrained gpt tokenizer: tokenizer=GPTTokenizer(\n",
      "  encoding=gpt-2\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  mergeable_ranks_size=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTTokenizer(\n",
       "  encoding=gpt-2\n",
       "  special_tokens={'<|endoftext|>': 50256}\n",
       "  eot_id=50256\n",
       "  mergeable_ranks_size=50256\n",
       "  merges_size=50000\n",
       "  vocab_size=50257\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import Tokenizer, PretrainedGPTTokenizerConfig\n",
    "\n",
    "tokenizer = await Tokenizer.load(\n",
    "    config=PretrainedGPTTokenizerConfig(encoding_or_model_name=\"gpt-2\"),\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929288e0-b406-4750-b979-fff930c07703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', 'Hello', ',', ' world', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"<|endoftext|>Hello, world!\"\n",
    "ids = tokenizer.encode(text, allowed_special_tokens=\"all\")\n",
    "[tokenizer.decode([id]) for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d781eab-110f-48aa-bf9d-270c72c110a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Hello, world!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "assert decoded_text == text\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f243dd-baaf-4055-83c8-e8b5a2e4b84f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c1702e-3dd9-449c-aac8-93ae721402c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:43:23 Loading pretrained GPT-2 model: config=type='pretrained_gpt_2' model_type=<PretrainedGPT2ModelType.GPT_2: 'gpt-2'> embd_dropout_p=None attn_dropout_p=None residual_dropout_p=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gpahal/Developer/python/projects/microgpt/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:43:24 Loading pretrained gpt tokenizer: config=type='pretrained_gpt' encoding_or_model_name='gpt-2'\n",
      "21:43:24 Pretrained gpt tokenizer encoding: encoding=gpt-2\n",
      "21:43:24 Loaded mergeable ranks\n",
      "21:43:24 Loaded pretrained gpt tokenizer: tokenizer=GPTTokenizer(\n",
      "  encoding=gpt-2\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  mergeable_ranks_size=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "21:43:24 Loaded model params: params={'max_seq_len': 1024, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'use_padded_vocab_size': False, 'use_rope': False, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.1, 'attn_dropout_p': 0.1, 'residual_dropout_p': 0.1, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
      "21:43:27 No. of parameters: 124.44M\n",
      "21:43:27 Loading Huggingface pretrained GPT-2 model: huggingface_model_name=gpt2\n",
      "21:43:28 Loaded Huggingface pretrained GPT-2 model\n",
      "21:43:28 Loaded pretrained GPT-2 model: model=Model(\n",
      "  device=cpu\n",
      "  params={'max_seq_len': 1024, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'use_padded_vocab_size': False, 'use_rope': False, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.1, 'attn_dropout_p': 0.1, 'residual_dropout_p': 0.1, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
      "  tokenizer=GPTTokenizer(\n",
      "  encoding=gpt-2\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  mergeable_ranks_size=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n",
      "  vocab_size=50257\n",
      "  padded_vocab_size=50257\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  device=cpu\n",
       "  params={'max_seq_len': 1024, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'use_padded_vocab_size': False, 'use_rope': False, 'rope_theta': 10000.0, 'is_rope_full_precision': True, 'embd_dropout_p': 0.1, 'attn_dropout_p': 0.1, 'residual_dropout_p': 0.1, 'init_std': None, 'init_residual_scaled_factor': 2.0}\n",
       "  tokenizer=GPTTokenizer(\n",
       "  encoding=gpt-2\n",
       "  special_tokens={'<|endoftext|>': 50256}\n",
       "  eot_id=50256\n",
       "  mergeable_ranks_size=50256\n",
       "  merges_size=50000\n",
       "  vocab_size=50257\n",
       ")\n",
       "  vocab_size=50257\n",
       "  padded_vocab_size=50257\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import Model, PretrainedGPT2ModelConfig, PretrainedGPT2ModelType\n",
    "\n",
    "model = await Model.load(\n",
    "    config=PretrainedGPT2ModelConfig(model_type=PretrainedGPT2ModelType.GPT_2),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4c73e6-4919-4b86-a77a-bb530239b651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, I\\'m a language model, because of nest things for sure!\"\\n\\nhe said.\\n\\n\"You'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, I'm a language model,\"\n",
    "generated_text = model.generate_text(text=text, max_new_tokens=16)\n",
    "assert len(generated_text) > len(text)\n",
    "assert generated_text.startswith(text)\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
