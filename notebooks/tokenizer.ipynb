{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f3666f-0f01-4f12-b437-92cfa54d2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c27710",
   "metadata": {},
   "source": [
    "# Load the pretrained tokenizer\n",
    "\n",
    "There is a pretrained tokenizer from the `pretrained/tokenizer` directory which was trained on the 125,000 rows of the `fineweb-edu` dataset. It has 50257 tokens including the end-of-text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bb30c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:42:32 Loading pretrained tokenizer: config=type='custom_trained' dir_path='/Users/gpahal/Developer/python/projects/microgpt/pretrained/tokenizer'\n",
      "21:42:32 Loaded pretrained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 50256}\n",
      "  eot_id=50256\n",
      "  merges_size=50000\n",
      "  vocab_size=50257\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(\n",
       "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
       "  special_tokens={'<|endoftext|>': 50256}\n",
       "  eot_id=50256\n",
       "  merges_size=50000\n",
       "  vocab_size=50257\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import Tokenizer, PretrainedTokenizerConfig\n",
    "\n",
    "tokenizer = await Tokenizer.load(\n",
    "    config=PretrainedTokenizerConfig(),\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ac1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " ' world',\n",
       " '!',\n",
       " \" I'm\",\n",
       " ' a',\n",
       " ' gener',\n",
       " 'ative',\n",
       " ' pre',\n",
       " '-trained',\n",
       " ' transformer',\n",
       " ' (',\n",
       " 'G',\n",
       " 'PT',\n",
       " ')']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, world! I'm a generative pre-trained transformer (GPT)\"\n",
    "ids = tokenizer.encode(text)\n",
    "[tokenizer.decode([id]) for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75dae253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, world! I'm a generative pre-trained transformer (GPT)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids=ids)\n",
    "assert decoded_text == text\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d692b-9a61-456d-97e4-13a1ed98a5c2",
   "metadata": {},
   "source": [
    "# Train a custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4471e1",
   "metadata": {},
   "source": [
    "To see how to train a custom tokenizer with larger huggingface datasets, see the `scripts/tokenizer/train_tokenizer.py` script.\n",
    "\n",
    "To train a very simplified model, follow the step given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929288e0-b406-4750-b979-fff930c07703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:42:32 Loading tokenizer trainer: config=tokenizer_config=UntrainedTokenizerConfig(type='untrained', split_pattern=None, special_tokens={'<|endoftext|>': 356}, eot_id=356) output_dir_path='trained_tokenizer' checkpointing_config=None vocab_size=357 data_sources=[TextDataSource(name=sample, text_len=1513)]\n",
      "21:42:32 Loading untrained tokenizer: config=type='untrained' split_pattern=None special_tokens={'<|endoftext|>': 356} eot_id=356\n",
      "21:42:32 Loaded untrained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 356}\n",
      "  eot_id=356\n",
      "  merges_size=0\n",
      "  vocab_size=257\n",
      ")\n",
      "21:42:32 Creating output directory: dir_path=trained_tokenizer\n",
      "21:42:32 Loaded non-checkpointed tokenizer trainer: tokenizer_trainer=TokenizerTrainer(\n",
      "  name=tokenizer\n",
      "  iteration=1\n",
      "  latest_run_iteration=1\n",
      "  output_dir_path=trained_tokenizer\n",
      "  checkpointing_config=None\n",
      "  tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 356}\n",
      "  eot_id=356\n",
      "  merges_size=0\n",
      "  vocab_size=257\n",
      ")\n",
      "  vocab_size=357\n",
      "  data_sources=[TextDataSource(name=sample, text_len=1513)]\n",
      ")\n",
      "21:42:32 Training tokenizer: existing_merges_count=0 num_merges=100\n",
      "21:42:32 Processing data sources: inputs_need_merging=False data_sources=[TextDataSource(name=sample, text_len=1513)]\n",
      "21:42:32 Processed data sources: num_merges=100 max_iterations=100 data_source_tokens={'sample': 1515} tokens=1837 counts_dict=272 starting_id=256\n",
      "21:42:32 Training:   0%|          | 0/100 [00:00<?, ?iters/s]\n",
      "21:42:32 Training:   1%|          | 1/100 [00:00<00:00, 512.00iters/s]\n",
      "21:42:32 Training:   2%|▏         | 2/100 [00:00<00:00, 836.10iters/s]\n",
      "21:42:32 Training:   3%|▎         | 3/100 [00:00<00:00, 1006.07iters/s]\n",
      "21:42:32 Training:   4%|▍         | 4/100 [00:00<00:00, 1158.73iters/s]\n",
      "21:42:32 Training:   5%|▌         | 5/100 [00:00<00:00, 1311.70iters/s]\n",
      "21:42:32 Training:   6%|▌         | 6/100 [00:00<00:00, 1429.31iters/s]\n",
      "21:42:32 Training:   7%|▋         | 7/100 [00:00<00:00, 1542.19iters/s]\n",
      "21:42:32 Training:   8%|▊         | 8/100 [00:00<00:00, 1648.79iters/s]\n",
      "21:42:32 Training:   9%|▉         | 9/100 [00:00<00:00, 1760.18iters/s]\n",
      "21:42:32 Training:  10%|█         | 10/100 [00:00<00:00, 1835.90iters/s]\n",
      "21:42:32 Training:  11%|█         | 11/100 [00:00<00:00, 1860.38iters/s]\n",
      "21:42:32 Training:  12%|█▏        | 12/100 [00:00<00:00, 1893.02iters/s]\n",
      "21:42:32 Training:  13%|█▎        | 13/100 [00:00<00:00, 1948.75iters/s]\n",
      "21:42:32 Training:  14%|█▍        | 14/100 [00:00<00:00, 1972.93iters/s]\n",
      "21:42:32 Training:  15%|█▌        | 15/100 [00:00<00:00, 2033.04iters/s]\n",
      "21:42:32 Training:  16%|█▌        | 16/100 [00:00<00:00, 2077.10iters/s]\n",
      "21:42:32 Training:  17%|█▋        | 17/100 [00:00<00:00, 2150.21iters/s]\n",
      "21:42:32 Training:  18%|█▊        | 18/100 [00:00<00:00, 2210.50iters/s]\n",
      "21:42:32 Training:  19%|█▉        | 19/100 [00:00<00:00, 2238.16iters/s]\n",
      "21:42:32 Training:  20%|██        | 20/100 [00:00<00:00, 2279.45iters/s]\n",
      "21:42:32 Training:  21%|██        | 21/100 [00:00<00:00, 2335.92iters/s]\n",
      "21:42:32 Training:  22%|██▏       | 22/100 [00:00<00:00, 2379.93iters/s]\n",
      "21:42:32 Training:  23%|██▎       | 23/100 [00:00<00:00, 2430.26iters/s]\n",
      "21:42:32 Training:  24%|██▍       | 24/100 [00:00<00:00, 2478.05iters/s]\n",
      "21:42:32 Training:  25%|██▌       | 25/100 [00:00<00:00, 2510.84iters/s]\n",
      "21:42:32 Training:  26%|██▌       | 26/100 [00:00<00:00, 2502.17iters/s]\n",
      "21:42:32 Training:  27%|██▋       | 27/100 [00:00<00:00, 2508.83iters/s]\n",
      "21:42:32 Training:  28%|██▊       | 28/100 [00:00<00:00, 2533.23iters/s]\n",
      "21:42:32 Training:  29%|██▉       | 29/100 [00:00<00:00, 2578.65iters/s]\n",
      "21:42:32 Training:  30%|███       | 30/100 [00:00<00:00, 2611.65iters/s]\n",
      "21:42:32 Training:  31%|███       | 31/100 [00:00<00:00, 2621.12iters/s]\n",
      "21:42:32 Training:  32%|███▏      | 32/100 [00:00<00:00, 2661.31iters/s]\n",
      "21:42:32 Training:  33%|███▎      | 33/100 [00:00<00:00, 2652.94iters/s]\n",
      "21:42:32 Training:  34%|███▍      | 34/100 [00:00<00:00, 2692.87iters/s]\n",
      "21:42:32 Training:  35%|███▌      | 35/100 [00:00<00:00, 2713.56iters/s]\n",
      "21:42:32 Training:  36%|███▌      | 36/100 [00:00<00:00, 2720.04iters/s]\n",
      "21:42:32 Training:  37%|███▋      | 37/100 [00:00<00:00, 2711.39iters/s]\n",
      "21:42:32 Training:  38%|███▊      | 38/100 [00:00<00:00, 2695.43iters/s]\n",
      "21:42:32 Training:  39%|███▉      | 39/100 [00:00<00:00, 2717.60iters/s]\n",
      "21:42:32 Training:  40%|████      | 40/100 [00:00<00:00, 2736.55iters/s]\n",
      "21:42:32 Training:  41%|████      | 41/100 [00:00<00:00, 2759.06iters/s]\n",
      "21:42:32 Training:  42%|████▏     | 42/100 [00:00<00:00, 2787.00iters/s]\n",
      "21:42:32 Training:  43%|████▎     | 43/100 [00:00<00:00, 2814.66iters/s]\n",
      "21:42:32 Training:  44%|████▍     | 44/100 [00:00<00:00, 2842.15iters/s]\n",
      "21:42:32 Training:  45%|████▌     | 45/100 [00:00<00:00, 2859.15iters/s]\n",
      "21:42:32 Training:  46%|████▌     | 46/100 [00:00<00:00, 2876.11iters/s]\n",
      "21:42:32 Training:  47%|████▋     | 47/100 [00:00<00:00, 2892.33iters/s]\n",
      "21:42:32 Training:  48%|████▊     | 48/100 [00:00<00:00, 2904.18iters/s]\n",
      "21:42:32 Training:  49%|████▉     | 49/100 [00:00<00:00, 2915.64iters/s]\n",
      "21:42:32 Training:  50%|█████     | 50/100 [00:00<00:00, 2923.59iters/s]\n",
      "21:42:32 Training:  51%|█████     | 51/100 [00:00<00:00, 2910.61iters/s]\n",
      "21:42:32 Training:  52%|█████▏    | 52/100 [00:00<00:00, 2906.62iters/s]\n",
      "21:42:32 Training:  53%|█████▎    | 53/100 [00:00<00:00, 2914.96iters/s]\n",
      "21:42:32 Training:  54%|█████▍    | 54/100 [00:00<00:00, 2933.50iters/s]\n",
      "21:42:32 Training:  55%|█████▌    | 55/100 [00:00<00:00, 2921.01iters/s]\n",
      "21:42:32 Training:  56%|█████▌    | 56/100 [00:00<00:00, 2940.09iters/s]\n",
      "21:42:32 Training:  57%|█████▋    | 57/100 [00:00<00:00, 2960.72iters/s]\n",
      "21:42:32 Training:  58%|█████▊    | 58/100 [00:00<00:00, 2974.83iters/s]\n",
      "21:42:32 Training:  59%|█████▉    | 59/100 [00:00<00:00, 2982.82iters/s]\n",
      "21:42:32 Training:  60%|██████    | 60/100 [00:00<00:00, 2994.61iters/s]\n",
      "21:42:32 Training:  61%|██████    | 61/100 [00:00<00:00, 2995.30iters/s]\n",
      "21:42:32 Training:  62%|██████▏   | 62/100 [00:00<00:00, 3011.89iters/s]\n",
      "21:42:32 Training:  63%|██████▎   | 63/100 [00:00<00:00, 3028.83iters/s]\n",
      "21:42:32 Training:  64%|██████▍   | 64/100 [00:00<00:00, 3027.73iters/s]\n",
      "21:42:32 Training:  65%|██████▌   | 65/100 [00:00<00:00, 3048.05iters/s]\n",
      "21:42:32 Training:  66%|██████▌   | 66/100 [00:00<00:00, 3065.64iters/s]\n",
      "21:42:32 Training:  67%|██████▋   | 67/100 [00:00<00:00, 3082.86iters/s]\n",
      "21:42:32 Training:  68%|██████▊   | 68/100 [00:00<00:00, 3098.25iters/s]\n",
      "21:42:32 Training:  69%|██████▉   | 69/100 [00:00<00:00, 3105.29iters/s]\n",
      "21:42:32 Training:  70%|███████   | 70/100 [00:00<00:00, 3106.27iters/s]\n",
      "21:42:32 Training:  71%|███████   | 71/100 [00:00<00:00, 3099.36iters/s]\n",
      "21:42:32 Training:  72%|███████▏  | 72/100 [00:00<00:00, 3087.99iters/s]\n",
      "21:42:32 Training:  73%|███████▎  | 73/100 [00:00<00:00, 3074.73iters/s]\n",
      "21:42:32 Training:  74%|███████▍  | 74/100 [00:00<00:00, 3075.76iters/s]\n",
      "21:42:32 Training:  75%|███████▌  | 75/100 [00:00<00:00, 3085.41iters/s]\n",
      "21:42:32 Training:  76%|███████▌  | 76/100 [00:00<00:00, 3097.74iters/s]\n",
      "21:42:32 Training:  77%|███████▋  | 77/100 [00:00<00:00, 3107.97iters/s]\n",
      "21:42:32 Training:  78%|███████▊  | 78/100 [00:00<00:00, 3118.89iters/s]\n",
      "21:42:32 Training:  79%|███████▉  | 79/100 [00:00<00:00, 3128.36iters/s]\n",
      "21:42:32 Training:  80%|████████  | 80/100 [00:00<00:00, 3129.79iters/s]\n",
      "21:42:32 Training:  81%|████████  | 81/100 [00:00<00:00, 3121.51iters/s]\n",
      "21:42:32 Training:  82%|████████▏ | 82/100 [00:00<00:00, 3122.86iters/s]\n",
      "21:42:32 Training:  83%|████████▎ | 83/100 [00:00<00:00, 3129.60iters/s]\n",
      "21:42:32 Training:  84%|████████▍ | 84/100 [00:00<00:00, 3119.99iters/s]\n",
      "21:42:32 Training:  85%|████████▌ | 85/100 [00:00<00:00, 3123.85iters/s]\n",
      "21:42:32 Training:  86%|████████▌ | 86/100 [00:00<00:00, 3118.63iters/s]\n",
      "21:42:32 Training:  87%|████████▋ | 87/100 [00:00<00:00, 3120.07iters/s]\n",
      "21:42:32 Training:  88%|████████▊ | 88/100 [00:00<00:00, 3121.55iters/s]\n",
      "21:42:32 Training:  89%|████████▉ | 89/100 [00:00<00:00, 3129.71iters/s]\n",
      "21:42:32 Training:  90%|█████████ | 90/100 [00:00<00:00, 3139.50iters/s]\n",
      "21:42:32 Training:  91%|█████████ | 91/100 [00:00<00:00, 3140.20iters/s]\n",
      "21:42:32 Training:  92%|█████████▏| 92/100 [00:00<00:00, 3147.44iters/s]\n",
      "21:42:32 Training:  93%|█████████▎| 93/100 [00:00<00:00, 3161.10iters/s]\n",
      "21:42:32 Training:  94%|█████████▍| 94/100 [00:00<00:00, 3173.54iters/s]\n",
      "21:42:32 Training:  95%|█████████▌| 95/100 [00:00<00:00, 3180.65iters/s]\n",
      "21:42:32 Training:  96%|█████████▌| 96/100 [00:00<00:00, 3190.42iters/s]\n",
      "21:42:32 Training:  97%|█████████▋| 97/100 [00:00<00:00, 3198.56iters/s]\n",
      "21:42:32 Training:  98%|█████████▊| 98/100 [00:00<00:00, 3193.65iters/s]\n",
      "21:42:32 Training:  99%|█████████▉| 99/100 [00:00<00:00, 3207.30iters/s]\n",
      "21:42:32 Training: 100%|██████████| 100/100 [00:00<00:00, 3218.64iters/s]\n",
      "21:42:32 Saving tokenizer trainer output: dir_path=trained_tokenizer\n",
      "21:42:32 Saving tokenizer params: file_path=trained_tokenizer/tokenizer.json\n",
      "21:42:32 Saved tokenizer params\n",
      "21:42:32 Saving tokenizer vocabulary: file_path=trained_tokenizer/tokenizer_vocab.json\n",
      "21:42:32 Saved tokenizer vocabulary\n",
      "21:42:32 Saved tokenizer trainer output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(\n",
       "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
       "  special_tokens={'<|endoftext|>': 356}\n",
       "  eot_id=356\n",
       "  merges_size=100\n",
       "  vocab_size=357\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import (\n",
    "    TextDataSource,\n",
    "    TokenizerTrainer,\n",
    "    TokenizerTrainerConfig,\n",
    "    UntrainedTokenizerConfig,\n",
    ")\n",
    "\n",
    "text = \"\"\"\n",
    "A generative pre-trained transformer (GPT) is a type of large language model (LLM)[1][2][3] and a prominent framework for generative artificial intelligence.[4][5] It is an artificial neural network that is used in natural language processing by machines.[6] It is based on the transformer deep learning architecture, pre-trained on large data sets of unlabeled text, and able to generate novel human-like content.[2][3] As of 2023, most LLMs had these characteristics[7] and are sometimes referred to broadly as GPTs.[8]\n",
    "\n",
    "The first GPT was introduced in 2018 by OpenAI.[9] OpenAI has released significant GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series.[10] Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4o, was released in May 2024.[11] Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.[1]\n",
    "\n",
    "The term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI,[12] and seven models created by Cerebras in 2023.[13] Companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce's \"EinsteinGPT\" (for CRM)[14] and Bloomberg's \"BloombergGPT\" (for finance).[15]\n",
    "\"\"\".strip()\n",
    "tokenizer_trainer = await TokenizerTrainer.load(\n",
    "    config=TokenizerTrainerConfig(\n",
    "        tokenizer_config=UntrainedTokenizerConfig(\n",
    "            special_tokens={\"<|endoftext|>\": 356},\n",
    "            eot_id=356,\n",
    "        ),\n",
    "        output_dir_path=\"trained_tokenizer\",\n",
    "        vocab_size=357,\n",
    "        data_sources=[TextDataSource(name=\"sample\", text=text)],\n",
    "    ),\n",
    ")\n",
    "tokenizer = await tokenizer_trainer.run()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d781eab-110f-48aa-bf9d-270c72c110a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'el',\n",
       " 'lo',\n",
       " ',',\n",
       " ' w',\n",
       " 'or',\n",
       " 'l',\n",
       " 'd',\n",
       " '!',\n",
       " ' ',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " ' a',\n",
       " ' gen',\n",
       " 'er',\n",
       " 'ati',\n",
       " 've',\n",
       " ' pre',\n",
       " '-t',\n",
       " 'rain',\n",
       " 'ed',\n",
       " ' t',\n",
       " 'ra',\n",
       " 'n',\n",
       " 's',\n",
       " 'for',\n",
       " 'm',\n",
       " 'er',\n",
       " ' (',\n",
       " 'GPT',\n",
       " ')']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, world! I'm a generative pre-trained transformer (GPT)\"\n",
    "ids = tokenizer.encode(text)\n",
    "[tokenizer.decode([id]) for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d3b08a-df90-479f-a1a8-6e49cce6cfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, world! I'm a generative pre-trained transformer (GPT)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "assert decoded_text == text\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ae103",
   "metadata": {},
   "source": [
    "# Load the custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a97565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:42:32 Loading custom trained tokenizer: config=type='custom_trained' dir_path='trained_tokenizer'\n",
      "21:42:32 Loaded custom trained tokenizer: tokenizer=Tokenizer(\n",
      "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
      "  special_tokens={'<|endoftext|>': 356}\n",
      "  eot_id=356\n",
      "  merges_size=100\n",
      "  vocab_size=357\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(\n",
       "  split_pattern=[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n",
       "  special_tokens={'<|endoftext|>': 356}\n",
       "  eot_id=356\n",
       "  merges_size=100\n",
       "  vocab_size=357\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from microgpt import Tokenizer, CustomTrainedTokenizerConfig\n",
    "\n",
    "tokenizer = await Tokenizer.load(\n",
    "    config=CustomTrainedTokenizerConfig(\n",
    "        dir_path=\"trained_tokenizer\",\n",
    "    ),\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2863201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, world! I'm a generative pre-trained transformer (GPT)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids=ids)\n",
    "assert decoded_text == text\n",
    "decoded_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
